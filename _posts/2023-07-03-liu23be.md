---
title: 'N$\textA^\text2$Q: Neural Attention Additive Model for Interpretable Multi-Agent
  Q-Learning'
openreview: oUeo2uG1AZ
abstract: Value decomposition is widely used in cooperative multi-agent reinforcement
  learning, however, its implicit credit assignment mechanism is not yet fully understood
  due to black-box networks. In this work, we study an interpretable value decomposition
  framework via the family of generalized additive models. We present a novel method,
  named Neural Attention Additive Q-learning (N$\text{A}^\text{2}$Q), providing inherent
  intelligibility of collaboration behavior. N$\text{A}^\text{2}$Q can explicitly
  factorize the optimal joint policy induced by enriching shape functions to model
  all possible coalition of agents into individual policies. Moreover, we construct
  the identity semantics to promote estimating credits together with the global state
  and individual value functions, where local semantic masks help us diagnose whether
  each agent captures the relevant-task information. Extensive experiments show that
  N$\text{A}^\text{2}$Q consistently achieves superior performance compared to different
  state-of-the-art methods on all challenging tasks, while yielding human-like interpretability.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: liu23be
month: 0
tex_title: "{N}$\\text{{A}}^\\text{2}${Q}: Neural Attention Additive Model for Interpretable
  Multi-Agent Q-Learning"
firstpage: 22539
lastpage: 22558
page: 22539-22558
order: 22539
cycles: false
bibtex_author: Liu, Zichuan and Zhu, Yuanyang and Chen, Chunlin
author:
- given: Zichuan
  family: Liu
- given: Yuanyang
  family: Zhu
- given: Chunlin
  family: Chen
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/liu23be/liu23be.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
